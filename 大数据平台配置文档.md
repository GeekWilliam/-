# 大数据平台配置文档

## 作者:GeekWilliam 2020-01-03 V1.0.1

> 个人主页:https://github.com/GeekWilliam/

## 使用前必读

1. 本文档会帮助您在CentOS7(Linux操作系统)下安装如下软件
   1. Hadoop、MYSQL、Hive、Scala、Spark、Sqoop、
2. 请确保您有Linux基础，因为这是流畅安装的保证！(如果您没有,那么非常抱歉请移步Linux基础教程)
3. 本文档不能处理所有的问题，文档作者会尽可能的收集错误到文档里面，为大家提供解决方案！
4. 如果您在配置过程中不慎出现了文档中没有的错误，请您一定先百度或Google！
5. 如果您遇到配置错误尝试自己解决问题(已经百度或Google或查看相关文档)仍然无法解决问题那么这时您就可以寻求他人或者本文档作者的帮助，但请记住一点，有效的提问往往是快速解决一个问题的开始

------

[TOC]



## 准备工作

1. 一台计算机配置如下(最低配置)

   1. 系统:Windows 10 
   2. CPU:i5-8250U
   3. 内存:8GB
   4. 硬盘空间20GB(推荐40GB)

2. 准备如下软件安装包和系统光盘镜像 (请确保您已经下载好对应版本)

   1. VMware Workstation Pro 14或更高版本
   2. CentOS-7-x86_64-DVD-1810.iso
   3. Xshell-6.0.0193p.exe或更高版本
   4. Xftp-6.0.0187p.exe或更高版本
   5. jdk-8u231-linux-x64.tar.gz
   6. mysql-connector-java-5.1.49-bin.jar
   7. hadoop-2.7.5.tar.gz
   8. mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz
   9. apache-hive-2.3.7-bin.tar.gz
   10. scala-2.13.3.tgz
   11. spark-2.3.0-bin-hadoop2.7.gz
   12. sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz

   ------

   

## 安装CentOS 7

1. 在安装中硬盘可以不设置分区
2. 选择安装软件包,选择SOFTWARE SELECTION时一定要选择Server with GUI (非常重要)
3. 设置用户名为st

> 安装教程:https://blog.csdn.net/u013168176/article/details/81144193

------

## 使用Xshell连接虚拟机

1. 安装并打开Xshell，开启虚拟机打开终端输入ifconfig查看虚拟机的ip地址
2. 在Xshell中输入ssh root@xxx (xxx为虚拟机ip地址)
3. 出现 [root@xxx]# 的标识既是成功

## 配置Hadoop + JDK

1. 修改计算机名  vim /etc/hostname

   1. CentOs-1 #设置主机名
      NETWORKING=yes #使用网络

2. 修改ip地址值变为静态

   1. cd /etc/sysconfig/network-scripts 
   2. vim ifcfg-ens33
   3. TYPE="Ethernet"
      	PROXY_METHOD="none"
         	BROWSER_ONLY="no"
         	BOOTPROTO="static"
         	DEFROUTE="yes"
         	IPV4_FAILURE_FATAL="no"
         	IPV6INIT="yes"
         	IPV6_AUTOCONF="yes"
         	IPV6_DEFROUTE="yes"
         	IPV6_FAILURE_FATAL="no"
         	IPV6_ADDR_GEN_MODE="stable-privacy"
         	NAME="ens33"
         	UUID="自己虚拟机UUID"
         	DEVICE="ens33"
         	ONBOOT="yes"
         	IPADDR="自己虚拟机ip地址"
         	PREFIX="24"
         	GATEWAY="255.255.255.0"
         	DNS1="8.8.8.8"
         	IPV6_PRIVACY="no"

3. 重启网卡服务 service network restart

4. 如果重启之后连接一会儿就断开
   是因为 NetworkManager 服务有冲突，这个好解决，直接关闭 NetworkManger 服务就好了， service NetworkManager stop，并且禁止开机启动 chkconfig NetworkManager off 。之后重启就好了

5. 修改网络配置 vim /etc/hosts

   1. 格式：IP地址    主机名称

6. 关闭防火墙

   1. systemctl stop firewalld.service #停止firewall
   2. systemctl disable firewalld.service #禁止firewall开机启动

7.  JDK 安装 

   1. 查看JDK rpm -qa | grep java
   2. 卸载JDK rpm -e --nodeps 安装的全部java版本
      （注意这些是不能卸载的
      python-javapackages-3.4.1-11.el7.noarch
      tzdata-java-2018e-3.el7.noarch
      javapackages-tools-3.4.1-11.el7.noarch）

8. JDK安装

   1. 编辑网络访问配置文件 vim /etc/hostname

9. 进入JDK安装目录 cd /home/st (若您安装CentOS时用户名不为st请修改为您填写的用户名)

   1. 解压安装文件 tar -zxvf jdk-8u231-linux-x64.tar.gz
   2. 配置全局环境变量  vim /etc/profile
   3. 配置参数
      export HADOOP_HOME=/home/st/hadoop
      exportPATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
      export JAVA_HOME=/home/st/jdk1.8.0_231
      export PATH=$PATH:$JAVA_HOME/bin
   4. 保存配置参数 source /etc/profile
   5. 检测是否安装成功 java -version
      (出现如下信息:java version "1.8.0_231"
      Java(TM) SE Runtime Environment (build 1.8.0_231-b11)
      Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode) 即可)

10. HADOOP 安装

    1. 进入JDK安装目录 cd /home/st
    2. 解压安装文件 tar -zxvf hadoop-2.7.5.tar.gz
    3. 修改目录名字 mv hadoop-2.7.5 hadoop
    4. 配置hadoop使用JDK路径 
       vim /home/st/hadoop/etc/hadoop/hadoop-env.sh
    5. 配置参数 export JAVA_HOME=/home/st/jdk1.8.0_231
       export PATH=$PATH:$JAVA_HOME/bin
    6. 配置核心组件文件 
       vim /home/st/hadoop/etc/hadoop/core-site.xml
       1. 配置参数
       2. <property>
          <name>fs.defaultFS</name>
          <value>hdfs://CentOs-1:9000</value>
          </property>
    7. hdfs访问管理配置 
       vim /home/st/hadoop/etc/hadoop/hdfs-site.xml
       1. 配置参数
       2. <property>
          <name>dfs.replication</name>
          <value>1</value></property>
          <property>
          <name>dfs.namenode.name.dir</name>
          <value>file:///home/st/hadoop/hadoopdb/data/dfs/name</value>
          </property> 
          <property>
          <name>dfs.datanode.data.dir</name>
          <value>file:///home/st/hadoop/hadoopdb/data/dfs/data</value>
          </property> 
          <property> 
          <name>dfs.permissions</name>
          <value>false</value>
          </property>     
          <property>
          <name>dfs.namenode.http-address</name>
          <value>CentOs-1:50070</value>
          </property>
       3. 新建hadoop数据存放目录
          1. cd /home/st/hadoop 
          2. mkdir hadoopdb
       4. yarn参数配置 vim /home/st/hadoop/etc/hadoop/yarn-site.xml
          1. 配置参数
          2. <property>
             <name>yarn.nodemanager.aux-services</name>
             <value>mapreduce_shuffle</value>
             </property>
             <property>
             <name>yarn.resourcemanager.address</name>
             <value> CentOs-1:8032</value>
             </property>
             <property>
             <name>yarn.resourcemanager.scheduler.address</name>
             <value> CentOs-1:8030</value>
             </property>
             <property>
             <name>yarn.resourcemanager.resource-tracker.address</name>
             <value> CentOs-1:8031</value>
             </property>
             <property>
             <name>yarn.resourcemanager.admin.address</name>
             <value> CentOs-1:8033</value>
             </property>
             <property>
             <name>yarn.resourcemanager.webapp.address</name>
             <value> CentOs-1:8088</value>
             </property>
       5. mapreduce管理应用程序配置
          1. cp /home/st/hadoop/etc/hadoop/mapred-site.xml.template /home/st/hadoop/etc/hadoop/mapred-site.xml
          2. vim /home/st/hadoop/etc/hadoop/mapred-site.xml
          3.  配置参数
          4. <property>
             <name>mapreduce.framework.name</name>
             <value>yarn</value>
             </property>
       6.  hadoop集群节点表配置 vim /home/st/hadoop/etc/hadoop/slaves
          1. 配置参数 CentOs-1
       7.  hadoop集群启动环境变量配置
          1. cd /home/st/hadoop 
          2. vim ~/.bash_profile
          3. 配置参数
             1. #HADOOP
                export HADOOP_HOME=/home/st/hadoop
                export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
             2. 使环境变量生效 source ~/.bash_profile

11. hadoop集群管理

    1. 文件系统初始化 hadoop namenode -format
    2.  hadoop集群启动 1 cd /home/st/hadoop/sbin  2 start-all.sh
    3. JPS查看启动服务 DataNode;NodeManager;Jps;NameNode;ResourceManager;SecondaryNameNode
    4. WEB页面hadoop集群管理
       1. http://自己虚拟机的ip地址:8088 or  http://自己虚拟机的ip地址:50070 ifconfig 查看主机ip地址 填上对应端口号即可在web页面访问

12. 其他问题

    1. 问题描述：
       终端执行 ./start-yarn.sh
       starting yarn daemons
       Error: Cannot find configuration directory: /etc/hadoop
       Error: Cannot find configuration directory: /etc/hadoop

    2. 是找不到目录的原因，通过阅读相应的shell脚本可以找到解决方案~

       解决方法：

       在hadoop-env.sh 配置一条hadoop配置文件所在目录
       export HADOOP_CONF_DIR=/home/st/hadoop/etc/hadoop/
       source  hadoop-env.sh  

------

## MySQL数据库安装配置

1. 卸载系统自带的Mariadb
   1. rpm -qa|grep mariadb
   2. rpm -e --nodeps 对应版本的mariadb
2. MySql安装
   1. 说明：最好以管理员身份，安装默认目录/usr/local
   2. 解压mysql安装文件: tar zxvf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz
   3. 修改目录名: mv mysql-5.7.24-linux-glibc2.12-x86_64 mysql 
   4. 创建数据存放目录
      1. cd /usr/local/mysql
      2. mkdir data 
   5. 创建mysql用户组及mysql用户
      1. groupadd mysql
      2. useradd -r -g mysql mysql
      3. passwd mysql (会提示输入密码,输入自己熟悉的密码即可)
   6. 更改mysql目录下所有的目录及文件夹所属的用户组和用户，以及权限
      1. chown -R mysql:mysql /usr/local/mysql
      2. chmod -R 775 /usr/local/mysql
   7. 更改 my.cnf配置文件
      1. vim /etc/my.cnf
      2. [mysqld]
         socket=/var/lib/mysql/mysql.sock
         basedir=/usr/local/mysql
         #设置mysql数据库的数据的存放目录
         datadir=/usr/local/mysql/data
         #允许最大连接数
         max_connections=200
         #服务端使用的字符集默认为8比特编码的latin1字符集
         character-set-server=utf8
         #创建新表时将使用的默认存储引擎
         default-storage-engine=INNODB
         lower_case_table_names=1
         max_allowed_packet=16M
         [client]
         port=3306
         socket=/var/lib/mysql/mysql.sock
   8. 创建上步用到的目录，并赋权限
      1. mkdir /var/lib/mysql
      2. sudo chown -R mysql:mysql /var/lib/mysql
   9. 建立软连接
      1. ln -s /usr/local/mysql/bin/mysql  /usr/local/bin
      2. ln -s /usr/local/mysql/bin/mysqladmin  /usr/local/bin
      3. ln -s /usr/local/mysql/bin/mysqld_safe  /usr/local/bin
   10. 数据库初始化
       1. cd /usr/local/mysql
       2. bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data
          备注：一定将初始化密码完整拷贝备用
   11. 开机自动启动设置
       1. cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql
       2. chmod +x /etc/init.d/mysql 
       3. chkconfig --add mysql //添加服务
       4. chkconfig --list mysql //检查是否添加成功，显示6中选项为成功
   12. 加入环境变量 
       1. vim /etc/profile
       2. export MYSQL_HOME=/usr/local/mysql
       3. export PATH=$PATH:$MYSQL_HOME/bin
       4. source /etc/profile
   13. 服务启动 service mysql start
   14. 客户端登录系统 mysql -u root -p
   15. 修改初始登录密码 set password for root@localhost = password('Geek1999**');
   16. 创建hive数据库 
       1. create database hive;
       2. show databases;
   17. root用户允许远程链接
       1. use mysql; 打开数据库
       2. update user set host ='%' where user ='root'; 开启远程链接权限
       3. FLUSH PRIVILEGES; 刷新权限
       4. select user,host from mysql.user; 查看权限

------

## Hive 安装配置

1. 下载解压apache-hive-2.3.7-bin.tar.gz 至/usr/local，并重命名为hive
   1. cd /usr/loca
   2. tar -zxvf  apache-hive-2.3.7-bin.tar.gz
   3. mv apache-hive-2.3.7-bin.tar.gz hive
2. 修改环境变量
   1. vim /etc/profile
   2. export HIVE_HOME=/usr/local/hive
      export PATH=$PATH:$HIVE_HOME/bin
   3. 使环境变量生效 source /etc/profile
3. 链接mysql配置
   1. cd /usr/local/hive/conf
   2. cp hive-default.xml.template  hive-site.xml
   3. vim hive-site.xml (进去之后按住Shift+g)
   4. <property>
              <name>javax.jdo.option.ConnectionUserName</name>
              <value>root</value>
          </property>
          <property>
              <name>javax.jdo.option.ConnectionPassword</name>
              <value>Geek1999**</value>
          </property>
         <property>
              <name>javax.jdo.option.ConnectionURL</name>
              <value>jdbc:mysql://192.168.217.138:3306/hive</value>
          </property>
          <property>
              <name>javax.jdo.option.ConnectionDriverName</name>
              <value>com.mysql.jdbc.Driver</value>
          </property>
4. 拷贝mysql驱动
   1. 将mysql-connector-java-5.1.49-bin.jar 上传至 /usr/local/
   2. cp mysql-connector-java-5.1.49-bin.jar hive/lib
   3. cd /usr/local/hive/bin
   4. 源数据初始化 bash schematool -dbType mysql -initSchema
5. HIVE使用
   1. 使用hive前，必须启动hadoop
   2. 文件系统初始化 hadoop namenode -format 如果已经初始化忽略这一步
   3. hadoop集群启动
      1. cd /home/st/hadoop/sbin
      2. start-all.sh
6. 其他问题
   1. 问题描述：若 hive 启动报错java.net.URISyntaxException: Relative path in absolute URI: {system:java.io.tmpdir%7D/%7Bsystem:user.name%7D
      	编辑配置hive文件hive-site.xml
   2. 解决方案：将含有"${system:Java.io.tmpdir}"字段的<value>标签内容全部替换成"/hive/tmp"
      1. 打开hive-site.xml 按住"/"键 填写tmpdir关键字搜索替换

------

## Spark 安装配置

1. 前提：hadoop安装配置成功
2. 安装scala
   1. cd /usr/local 使用Xftp 6上传 scala-2.13.3.tgz 到 /usr/local
   2. 配置环境变量 vim ~/.bashrc
      1.  export SCALA_HOME=/usr/local/scala-2.13.3
      2. export PATH=${SCALA_HOME}/bin:$PATH
   3. 使环境变量生效 source ~/.bashrc
   4. 验证是否成功安装 scala -version
3. 安装Spark
   1. 上传spark-2.3.0-bin-hadoop2.7.gz  /usr/local 
   2. 解压 tar zxvf spark-2.3.0-bin-hadoop2.7.gz
   3. 改名 mv spark-2.3.0-bin-hadoop2.7 spark
   4. 配置环境变量 vim /etc/profile
      1. export SPARK_HOME=/usr/local/spark
      2. export PATH=$PATH:$SPARK_HOME/bin
   5. 使环境变量生效 source /etc/profile
   6. cd /usr/local/spark/conf
   7. cp spark-env.sh.template spark-env.sh
   8. vim spark-env.sh
   9. export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
   10. vim /home/zhaosc/hadoop/etc/hadoop/yarn-site.xml  //修改yarn-site文件，防止内存不足引起问题
       1. <property>
          <name>yarn.nodemanager.pmem-check-enabled</name>
          <value>false</value>
          </property>
          <property>
          <name>yarn.nodemanager.vmem-check-enabled</name>
          <value>false</value>
          </property>
   11. scp yarn-site.xml root@192.168.11:/home/st/hadoop/etc/hadoop/ yarn-site.xml分发到其它节点  （分布式才使用）
   12.  启动spark 和 hadoop 
       1. cd /usr/local/spark/sbin
       2. ./start-all.sh
       3. 使用jps查看，会看到master服务
   13.  在spark/sbin 下启动spark客户端工具
       1. spark-shell --master yarn --deploy-mode client
   14. 查看spark管理界面
       1. http://X:8080/cluster (X为虚拟机的静态IP地址) 

------

## Sqoop 安装配置

1. hadoop安装配置好
   hive安装配置好
   mysql安装配置好
2. Sqoop安装
   1. 将sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz上传到 /usr/local
   2. cd  /usr/local
   3. tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
   4. mv sqoop-1.4.7.bin__hadoop-2.6.0 sqoop
   5. cp mysql-connector-java-5.1.49-bin.jar sqoop/lib (mysql驱动包必须提前拷贝到/usr/local下)
   6. 配置环境变量 
      1. vim /etc/profile
      2. export SQOOP_HOME=/usr/local/sqoop
         export PATH=$PATH:$SQOOP_HOME/bin
      3. 使环境变量生效 source /etc/profile
   7. cd /usr/local/sqoop/conf 
   8. mv sqoop-env-template.sh sqoop-env.sh
   9. vim sqoop-env.sh
   10. export HADOOP_COMMON_HOME=/home/st/hadoop
       export HADOOP_MAPRED_HOME=/home/st/hadoop
       export HIVE_HOME=/usr/local/hive
   11. 测试sqoop是否安装成功
       1. sqoop list-tables -connect jdbc:mysql://自己虚拟机的ip地址:3306/hive --username root --password Geek1999**
          （ip地址填自己的虚拟机地址，后面的是mysql 用户名和密码 对应填写即可）

